{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:19:00.981180611Z",
     "start_time": "2023-11-10T18:19:00.977584288Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_date=Timestamp('2023-11-10 18:00:00')\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "current_date = pd.to_datetime(datetime.utcnow()).floor('H') # - timedelta(hours=1)\n",
    "print(f'{current_date=}')\n",
    "# current_date = pd.Timestamp('2023-02-28 09:00:00')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:19:01.987614594Z",
     "start_time": "2023-11-10T18:19:01.795154542Z"
    }
   },
   "id": "1ac433f818fb0d0f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/178327\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "2023-11-10 19:21:27,225 INFO: Feature view already exists, skipping creation.\n",
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/178327\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Finished: Reading data from Hopsworks, using ArrowFlight (9.64s) from Hopsworks, using ArrowFlight.   Reading data from Hopsworks, using ArrowFlight...   Reading data from Hopsworks, using ArrowFlight.   Reading data from Hopsworks, using ArrowFlight...   Reading data from Hopsworks, using ArrowFlight.   Reading data from Hopsworks, using ArrowFlight...   Reading data from Hopsworks, using ArrowFlight.   Reading data from Hopsworks, using ArrowFlight...   \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Time-series data is not complete. Make sure your feature pipeline is up and running.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_batch_of_features_from_store\n\u001B[0;32m----> 3\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[43mload_batch_of_features_from_store\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurrent_date\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/learning-machine-learning/mlops/taxi_demand_predictor/src/inference.py:72\u001B[0m, in \u001B[0;36mload_batch_of_features_from_store\u001B[0;34m(current_date)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# validate we are not missing data in the feature store\u001B[39;00m\n\u001B[1;32m     71\u001B[0m location_ids \u001B[38;5;241m=\u001B[39m ts_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_location_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munique()\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(ts_data) \u001B[38;5;241m==\u001B[39m config\u001B[38;5;241m.\u001B[39mN_FEATURES \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(location_ids), \\\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime-series data is not complete. Make sure your feature pipeline is up and running.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# sort data by location and time\u001B[39;00m\n\u001B[1;32m     76\u001B[0m ts_data\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_location_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_hour\u001B[39m\u001B[38;5;124m'\u001B[39m], inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Time-series data is not complete. Make sure your feature pipeline is up and running."
     ]
    }
   ],
   "source": [
    "from src.inference import load_batch_of_features_from_store\n",
    "\n",
    "features = load_batch_of_features_from_store(current_date)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:21:40.456146286Z",
     "start_time": "2023-11-10T18:21:24.886389270Z"
    }
   },
   "id": "d4008b9f0275289f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/178327\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/5d669e40-4353-452c-bb90-d4bdeca20597/taxi_demand_predictor_next_hour/1/model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      2\u001B[0m     load_model_from_registry,\n\u001B[1;32m      3\u001B[0m     get_model_predictions\n\u001B[1;32m      4\u001B[0m )\n\u001B[0;32m----> 6\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_model_from_registry\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m predictions \u001B[38;5;241m=\u001B[39m get_model_predictions(model, features)\n",
      "File \u001B[0;32m~/workspace/learning-machine-learning/mlops/taxi_demand_predictor/src/inference.py:111\u001B[0m, in \u001B[0;36mload_model_from_registry\u001B[0;34m()\u001B[0m\n\u001B[1;32m    105\u001B[0m model \u001B[38;5;241m=\u001B[39m model_registry\u001B[38;5;241m.\u001B[39mget_model(\n\u001B[1;32m    106\u001B[0m     name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mMODEL_NAME,\n\u001B[1;32m    107\u001B[0m     version\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mMODEL_VERSION,\n\u001B[1;32m    108\u001B[0m )\n\u001B[1;32m    110\u001B[0m model_dir \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mdownload()\n\u001B[0;32m--> 111\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mjoblib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m)\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/src-4E75Pf6i-py3.11/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filename, mmap_mode)\u001B[0m\n\u001B[1;32m    648\u001B[0m         obj \u001B[38;5;241m=\u001B[39m _unpickle(fobj)\n\u001B[1;32m    649\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 650\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    651\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m _read_fileobject(f, filename, mmap_mode) \u001B[38;5;28;01mas\u001B[39;00m fobj:\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fobj, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    653\u001B[0m                 \u001B[38;5;66;03m# if the returned file object is a string, this means we\u001B[39;00m\n\u001B[1;32m    654\u001B[0m                 \u001B[38;5;66;03m# try to load a pickle file generated with an version of\u001B[39;00m\n\u001B[1;32m    655\u001B[0m                 \u001B[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/tmp/5d669e40-4353-452c-bb90-d4bdeca20597/taxi_demand_predictor_next_hour/1/model.pkl'"
     ]
    }
   ],
   "source": [
    "from src.inference import (\n",
    "    load_model_from_registry,\n",
    "    get_model_predictions\n",
    ")\n",
    "\n",
    "model = load_model_from_registry()\n",
    "predictions = get_model_predictions(model, features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:21:01.402406235Z",
     "start_time": "2023-11-10T18:20:26.169528251Z"
    }
   },
   "id": "d06bf226336df6ff"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mpredictions\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_hour\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m current_date\n\u001B[1;32m      2\u001B[0m predictions\n",
      "\u001B[0;31mNameError\u001B[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions['pickup_hour'] = current_date\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:21:07.827437749Z",
     "start_time": "2023-11-10T18:21:07.806195245Z"
    }
   },
   "id": "6e937873afd279aa"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/178327\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "from src.feature_store_api import get_feature_store\n",
    "import src.config as config\n",
    "\n",
    "# connect to the feature group\n",
    "feature_group = get_feature_store().get_or_create_feature_group(\n",
    "    name=config.FEATURE_GROUP_MODEL_PREDICTIONS,\n",
    "    version=1,\n",
    "    description=\"Predictions generate by our production model\",\n",
    "    primary_key = ['pickup_location_id', 'pickup_hour'],\n",
    "    event_time='pickup_hour',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:21:15.631965379Z",
     "start_time": "2023-11-10T18:21:14.052834165Z"
    }
   },
   "id": "893c32491eb64ad4"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m feature_group\u001B[38;5;241m.\u001B[39minsert(\u001B[43mpredictions\u001B[49m, write_options\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwait_for_job\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m})\n",
      "\u001B[0;31mNameError\u001B[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "feature_group.insert(predictions, write_options={\"wait_for_job\": False})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:21:19.996016578Z",
     "start_time": "2023-11-10T18:21:19.972516872Z"
    }
   },
   "id": "232b264d0ca7f8ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "78dfc45b81dda661"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
